\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{amsmath, amssymb, amsthm, amsfonts,cite,alltt,clrscode}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{accents}
\usepackage[list=true,listformat=simple]{subcaption}
\usepackage{color,colortbl}
\definecolor{almond}{rgb}{0.94, 0.87, 0.8}
\usepackage{hyperref}

% For maintaining extended abstract and full versions of a paper.
% Usage: \ifabstract short text [\else long  text] \fi
%        \iffull     long  text [\else short text] \fi
% Uncomment the line ``\abstractfalse'' to enable the full version.
\newif\ifabstract
%\abstracttrue
\abstractfalse
\newif\iffull
\ifabstract \fullfalse \else \fulltrue \fi

\ifabstract  %squish
\usepackage{times}

% Compact mainlevel section titles, using \paragraph's while keeping numbering,
% but without \appendix support.
{\makeatletter
 \gdef\section{\@ifnextchar*\section@star\section@normal}
 \gdef\section@normal#1{\refstepcounter{section}%
   \paragraph{\arabic{section}\hbox{~~}#1.}%
   \addcontentsline{toc}{section}{\protect\numberline{\arabic{section}}{#1}}}
 \gdef\section@star*#1{\paragraph{#1.}}}

% Compact subsection titles, using \paragraph's while keeping numbering.
{\makeatletter
 \gdef\subsection{\@ifnextchar*\subsection@star\subsection@normal}
 \gdef\subsection@normal#1{\refstepcounter{subsection}%
   \paragraph{\thesubsection\hbox{~~}#1.}%
   \addcontentsline{toc}{subsection}{\protect\numberline{\thesubsection}{#1}}}
 \gdef\subsection@star*#1{\paragraph{#1.}}}

% Redefine paragraph to have no leading space, but rather indent.
%{\makeatletter
% \gdef\paragraph{\@startsection{paragraph}{4}{\parindent}{-0pt}{-1em}
%   {\normalfont\normalsize\bfseries}}}

% Tighter version of just \paragraph.
\newlength\aboveparagraphskip
\aboveparagraphskip=3.25ex plus 1ex minus .2ex
\newlength\belowparagraphskip
\belowparagraphskip=-1em
\makeatletter
\def\paragraph{\@startsection{paragraph}{4}{\z@}{-\aboveparagraphskip}%
                 {\belowparagraphskip}{\normalfont\normalsize\bfseries}}
\makeatother
\aboveparagraphskip=.5ex plus .5ex minus .25ex

\fi

\urlstyle{same}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\ul}[1] {\underline{#1}}

\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

% Complex \xxx for making notes of things to do.  Use \xxx{...} for general
% notes, and \xxx[who]{...} if you want to blame someone in particular.
% Puts text in brackets and in bold font, and normally adds a marginpar
% with the text ``xxx'' so that it is easy to find.  On the other hand, if
% the comment is in a minipage, figure, or caption, the xxx goes in the text,
% because marginpars are not possible in these situations.
{\makeatletter
 \gdef\xxxmark{%
   \expandafter\ifx\csname @mpargs\endcsname\relax % in minipage?
     \expandafter\ifx\csname @captype\endcsname\relax % in figure/caption?
       \marginpar{xxx}% not in a caption or minipage, can use marginpar
     \else
       xxx % notice trailing space
     \fi
   \else
     xxx % notice trailing space
   \fi}
 \gdef\xxx{\@ifnextchar[\xxx@lab\xxx@nolab}
 \long\gdef\xxx@lab[#1]#2{\textbf{[\xxxmark #2 ---{\sc #1}]}}
 \long\gdef\xxx@nolab#1{\textbf{[\xxxmark #1]}}
 % This turns them off:
 %\long\gdef\xxx@lab[#1]#2{}\long\gdef\xxx@nolab#1{}%
}

{\catcode`\^^M=\active \gdef\tablines{\catcode`\^^M=\active \def^^M{\\}}}
\newenvironment{pcode}{\tablines \topsep=0pt \partopsep=0pt \tabbing
   MM\=MM\=MM\=MM\=MM\=MM\=MM\=MM\=MM\=MM\=MM\=MM\=MM\=\kill}
  {\endtabbing\vspace*{-0\baselineskip}}
{\makeatletter \gdef\lasttab{\ifnum \@curtab<\@hightab \>\lasttab\fi}}

% Compact list environments.  Note that itemize* and enumerate* use the
% same counters and symbols as the usual itemize and enumerate environments.
\def\compactify{\itemsep=0pt \topsep=0pt \partopsep=0pt \parsep=0pt}
\let\latexusecounter=\usecounter
\newenvironment{itemize*}
  {\begin{itemize}\compactify}
  {\end{itemize}}
\newenvironment{enumerate*}
  {\def\usecounter{\compactify\latexusecounter}
   \begin{enumerate}}
  {\end{enumerate}\let\usecounter=\latexusecounter}
\newenvironment{description*}
  {\begin{description}\compactify}
  {\end{description}}

% Put figures and text together
\def\textfraction{0.01}
\def\topfraction{0.99}
\def\dbltopfraction{0.99}
\def\bottomfraction{0.99}
\def\floatpagefraction{0.99}
\def\dblfloatpagefraction{0.99}
\def\dbltopnumber{100}

% Fonts
\def\id#1{\textit{#1}}
\def\proc#1{\textsc{#1}}
\let\epsilon=\varepsilon
\let\keyw=\textbf

%\newcommand{\lone}{l_{1/2}}
%\newcommand{\ltwo}{l_{2/2}}
\newcommand{\lone}{L[0\mathbin{:}\frac{n}{2}]}
\newcommand{\ltwo}{L[\frac{n}{2}+1\mathbin{:}n-1]}
\newcommand{\lsuper}{L}
\newcommand{\lrtos}{\lsuper[r\mathbin{:}s]}
\newcommand{\lstot}{\lsuper[s+1\mathbin{:}t]}
\newcommand{\lrtot}{\lsuper[r\mathbin{:}t]}
\newcommand{\lprime}{L'}
\newcommand{\lprtot}{L'[r:t]}

\begin{document}

\title{Discriminating Sound Textures}
\author{%
  Jayson Lynch%
    \thanks{MIT Computer Science and Artificial Intelligence Laboratory,
      32 Vassar Street, Cambridge, MA 02139, USA,
      \protect\url{{jaysonl}@mit.edu}.}
\and
  Eric Mannes%  
  \thanks{MIT Computer Science and Artificial Intelligence Laboratory,
      32 Vassar Street, Cambridge, MA 02139, USA,
      \protect\url{{mannes}@mit.edu}.}
}
\date{\today}
\maketitle
\begin{abstract}

We show sound textures are cool stuff.

\end{abstract}

\textbf{Keywords:} Machine Learning, Sound Textures, Audition

\thispagestyle{empty}
\setcounter{page}{-1}
\pagebreak

%\tableofcontents 
%\vfil
%\clearpage

%\listoffigures 
%\pagebreak

%\listoftables
%\pagebreak

\section{Introduction} 
This paper explores a new feature set for audio classification. Specifically, we look at a feature ensemble related to compressibility, sparsity, and temporal homogeneity in the problem of classifying audio signals as \emph{sound textures}. Sound textures, such as rain or crackling fire, are the result of many similar acoustic events. These sounds seem to form a cohesive auditory category which may be processed by the brain in a distinct fashion\cite{McDermott2011926, mcdermott2013summary}. Their study has partially been motivated by the analogous visual textures which comprise a significant body of literature. Tomita and Tsuji provide a book on the computational analysis of visual textures\cite{tomita2013computer}. The ideas that visual textures may be characterized by simple statistical features\cite{julesz1962visual} or atomistic units\cite{julesz1981textons} is carried over into some of the theory of sound textures. Recent work by McDermott and Simoncelli show many sound textures are well characterized by their time-averaged statistical properties\cite{McDermott2011926, mcdermott2013summary}. Saint-Arnaud's Master's Thesis attempts to extract the sound atoms that make up sound textures and use this as a basis for a classifier\cite{saint1995classification}. The thesis also discusses human perception of sound textures and the difficulties surrounding a good definition. A significant amount of the machine learning work in sound textures has been around synthesis often using wavelet hierarchies\cite{kersten2010sound} or sound atoms\cite{saint1995analysis}. Schwarz provides a general overview of methods as well as a classification scheme of the synthesis methods used\cite{schwarz2011state}.

McDermott and Simoncelli work shows that for many sound textures, the salient features that cause humans to classify these sounds are controlled by a number of time-averaged statistics of the sounds\cite{McDermott2011926, mcdermott2013summary}. This leads to an interesting question of why some sounds seem to be identified by time-averaged properties, when many other, such as speech or music, depend strongly on their temporal structure. McDermott et. al. conjecture that the `sparsity' or `compressibility' are related to the sounds that are perceived in this way. In addition, they mention that sound textures are `temporally homogeneous' without giving formal definitions of these terms. This paper seeks to build upon these ideas both to investigate new features for audio classification as well as to lend evidence toward the previous conjectures about sound texture perception. We provide several formal definitions of audio compressibility, sparsity, and temporal homogeneity in Section~\ref{sec:definitions}. These are then extracted as a feature set and are used in a machine learning algorithm for sound textures, described in Section~\ref{sec:methods}.

\section{Definitions}
\label{sec:definitions}
We provide working definitions for examples of three main descriptors: sparsity, compressibility, and temporal homogeneity. Since these characteristics can be captured in different ways, which are not always equivalent, we choose to work with multiple formal definitions.

Compressibility was the simplest to work out. We chose a standard lossy and lossless compression algorithm and measured the compression ratio for the .wav flies. In this case we used \xxx{what audio compression did we use?}

How to define sparsity was slightly less clear, since it usually refers to the quantity of zero entries with respect to some basis. To overcome this, we decided to use several natural representations. We looked at sparsity in the time domain with respect to the actual time series of the audio sample. We also looked at the sparsity in the frequency domain under both uniform and log-scale transforms. To be more precise, we took short-term Fourier transforms, constant q transforms, and Mel transforms of the time series (using the implementations in the Python library Librosa\cite{mcfee2015librosa}) and counted the ratio of entries near zero. Due to noise and precision error, we assumed an entry was empty if the magnitude of the value was less than $10^{-5}$. There are also algorithms for extracting low rank matrices assuming one has sampled from noisy data\cite{negahban2011estimation}. We would have liked to have implemented and run one of these algorithms, using the derived rank as a measure of sparsity, but we did not have time to extract this feature.

Finally temporal homogeneity refers to consistency in the signal over time. Obviously we can't have everything be exactly the same, so one must pick specific features with which to check consistency. A paper on visual textures provides a precise definition: $X$ is homogeneous with respect to the function $\Phi: \mathbb{R}^L \rightarrow \mathbb{R}$ with tolerance $\epsilon$ and probability $p$ if the average of $\Phi$ over a sample $x \in X$ is a good approximation to the average of $\Phi$ over all of $X$ \cite{portilla2000parametric}.
$$P_x(\mathrm{E}(\Phi(x)) - \mathrm{E}(\Phi(X)) < \epsilon) \geq p$$
In practice, this definition is slightly cumbersome to use, and requires picking either our probability or threshold arbitrarily. In the same spirit, we instead decide to use the variance of a given statistic over a sample. Thus we calculate the inhomogeneity of $X$ with respect to $\Phi$ as $$\Sigma_{x\in X} (\Phi(x) - \mathrm{E}(\Phi(X))^2$$
We additionally note that the original definition of homogeneity was given with respect to translations over a two dimensional space, whereas we take samples over a single time dimension.

In this paper we decided to use the first through fourth moments of the time series and various sub-bands of the audio signal. In particular we compute the short-term Fourier transforms, constant q transforms, and Mel transforms of the time series. We further computed the amplitude envelopes of these waveforms and took the corresponding short-term Fourier transforms, constant q transforms, or Mel transforms of the resulting amplitude envelope following the auditory model in \cite{McDermott2011926, mcdermott2013summary}. We would have also liked to look at the temporal homogeneity of the cross-correlation of the bands, capturing all of the statistics used in that paper.

\xxx{Give formal and informal definitions of the features we used}

\section{Methods}
\label{sec:methods}


All of our models were written in Python with the help of a number of external libraries. NumPy\cite{numpy} and Librosa\cite{mcfee2015librosa} were used for audio processing; SciPy\cite{scipy} provided a number of useful statistical methods; and scikit-learn\cite{scikit-learn} was used for our regression and classification.

\xxx{Describe how we set up our feature extraction. How we set up our learning. What our dataset is. Any other important process things.}

\section{Results}
\label{sec:results}
\xxx{What correlated and what didn't? What feature (ensemble) lead to a good classifier?}

\section{Conclusion}

\xxx{Speculate if these could be useful features in audio classification. Speculate about the implication to human audition. Give future directions.}

There are a number of future directions left open by this paper. First, the space of reasonable features has not been fully explored and may lead to yet better classification methods or more insight into the nature of sound textures. These features include: other standard compression algorithms, the entropy of the audio signals, the Kolmogorov complexity of the audio signal, rank estimation of the audio signal, temporal homogeneity of the cross-band correlations, and different sparsity estimation criteria. Another obvious next direction is the integration of these features with existing audio classification methods to attempt to improve performance in more complex settings. Third, there is still much to be understood about the nature of these features and what they can tell us about the audio files. The authors would have been interested to look at the correlation between features, as many of them should be highly related. Understanding when things like sparsity and compressibility differer might hold new insight into signal characteristics. Further, the audio dataset was limited and constrained in a number of ways, and it would be interesting to see how these features vary with over a wider set of sounds. Finally, there still remain important questions about the nature of human perception of sound textures. We showed that intuition about what makes up a sound texture is partially captured by some formal measures, but given none appeared to be necessary or sufficient, it seems that this issue may be more complex than originally stated. However, we also did not fully explore what might reasonably be meant by sparse and temporally homogeneous audio signals, so it is still possible that another definition will better capture what causes this qualitatively different auditory category.

\section*{Acknowledgments}

We thank Prof Josh McDermott for his support in answering our questions about their research and providing us with their dataset. We also thank the course staff of 6.867 for their instruction and support this semester. In particular, we appreciate Marzyeh Ghassemi advice and guidance in shaping our project plan.

% Decrease the space between bibliography items.
\let\realbibitem=\bibitem
\def\bibitem{\par \vspace{-1.5ex}\realbibitem}

% bibliography
\bibliographystyle{alpha}
\bibliography{SoundTexturesBib}
\end{document}

